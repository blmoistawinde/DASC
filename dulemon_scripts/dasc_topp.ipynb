{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "import re \n",
    "import json \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import defaultdict, Counter\n",
    "import torch \n",
    "import datasets\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "from bert_score import BERTScorer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import recover_sentence\n",
    "from model_multi_director import BartDASC\n",
    "from director_data_collator import DataCollatorForMultiBinaryDirector, DataCollatorForDynamicMaskMultiBinaryDirector\n",
    "from my_configs import BartMultiBinaryDecompDirectorConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"./datasets/dulemon/test.csv\"\n",
    "data_files = {}\n",
    "data_files[\"test\"] = test_file\n",
    "extension = test_file.split(\".\")[-1]\n",
    "raw_datasets = load_dataset(\n",
    "    extension,\n",
    "    data_files=data_files,\n",
    "    use_auth_token=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['anger',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'happiness',\n",
    " 'like',\n",
    " 'none',\n",
    " 'sadness',\n",
    " 'surprise']\n",
    "\n",
    "# 0/1/2: neutral/positive/negative\n",
    "emotion2big_class = [2, 2, 2, 1, 1, 0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(\"./datasets/dulemon/test.csv\")\n",
    "gt_texts = gt_df[\"target\"].values\n",
    "gender_labels = gt_df[\"Gender\"].values\n",
    "emotion_labels = gt_df[\"Emotion\"].values\n",
    "question_labels = gt_df[\"Question\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../train_dulemon_outputs/bart_dasc1\"\n",
    "config = BartMultiBinaryDecompDirectorConfig.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BartDASC.from_pretrained(model_path).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column = \"history\"\n",
    "target_column = \"target\"\n",
    "input_truncation_side = \"left\"\n",
    "max_source_length = 384\n",
    "max_target_length = 128\n",
    "padding = \"max_length\"\n",
    "ignore_pad_token_for_loss = True\n",
    "column_names = raw_datasets[\"test\"].column_names\n",
    "control_columns = ['Gender', 'Emotion', 'Question']\n",
    "num_control_pre_aspect = [3, 8, 2]\n",
    "num_controls = sum(num_control_pre_aspect)\n",
    "fp16 = True\n",
    "class_control_mapping = json.load(open(\"../aux_files/control_token_mapping_3aspect.json\", encoding=\"utf-8\"))\n",
    "used_control_mappings = {}\n",
    "extra_control_tokens = set([\"[speaker1]\", \"[speaker2]\"])\n",
    "for col in control_columns:\n",
    "    assert col in class_control_mapping\n",
    "    # sth like {0: \"\", 1: \"[positive]\", 2: \"[negative]\"}\n",
    "    for k in class_control_mapping[col]:\n",
    "        if k != \"\" and k.count(\"[\") == 1:  # do not add composed tokens like \"[-positive][-negative]\"\n",
    "            extra_control_tokens.add(k)\n",
    "    mapping0 = {v: k for k, v in class_control_mapping[col].items()}\n",
    "    used_control_mappings[col] = mapping0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # remove pairs where at least one record is None\n",
    "\n",
    "    inputs, targets, controls = [], [], []\n",
    "    for i in range(len(examples[input_column])):\n",
    "        if examples[input_column][i] and examples[target_column][i]:\n",
    "            inputs.append(examples[input_column][i])\n",
    "            targets.append(examples[target_column][i])\n",
    "            curr_control = [0] * num_controls\n",
    "            offset = 0\n",
    "            for control_column, num_control in zip(control_columns, num_control_pre_aspect):\n",
    "                control_label = int(examples[control_column][i])\n",
    "                curr_control[offset+control_label] = 1\n",
    "                offset += num_control\n",
    "            controls.append(curr_control)\n",
    "            \n",
    "    tokenizer.truncation_side = input_truncation_side\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    # labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "    tokenizer.truncation_side = 'right'\n",
    "    labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"controls\"] = controls\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = raw_datasets[\"test\"]\n",
    "predict_dataset = predict_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on prediction dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "label_pad_token_id = -100 if ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForMultiBinaryDirector(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8 if fp16 else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(predict_dataset, batch_size=32, num_workers=4, collate_fn=data_collator, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(model_path, \"infer_sampling/\")\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clf_infer_weight = -1  # no prior weighting, use +1 as weight\n",
    "all_outputs = []\n",
    "for batch_id, batch in enumerate(tqdm(data_loader)):\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    controls = batch[\"controls\"].to(model.device)\n",
    "    outputs = model.generate(input_ids, controls=controls, do_sample=True, max_length=128, top_p=0.5, num_return_sequences=1)\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    all_outputs.extend(outputs)\n",
    "all_outputs = list(map(recover_sentence, all_outputs))\n",
    "with open(out_dir+\"topp_0.5.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(all_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a2c21b6ce3a9a25fbf96bbbc230c069683ea074939a9001cdcf1028939befc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
