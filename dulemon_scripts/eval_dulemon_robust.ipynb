{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import json \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import defaultdict, Counter\n",
    "from utils import distinct_2\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(\"./datasets/dulemon/test_robust.csv\")\n",
    "gt_texts = gt_df[\"target\"].values\n",
    "gender_labels = gt_df[\"Gender\"].values\n",
    "emotion_labels = gt_df[\"Emotion\"].values\n",
    "sentiment_labels = gt_df[\"Sentiment\"].values\n",
    "question_labels = gt_df[\"Question\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_history = gt_df[\"history\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForSequenceClassification.from_pretrained('../sentiment_8class_no_emoji_clf1')\n",
    "tokenizer1 = AutoTokenizer.from_pretrained('../sentiment_8class_no_emoji_clf1')\n",
    "text_classification_sentiment = pipeline('sentiment-analysis', model=model1, tokenizer=tokenizer1, device=1)\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained('../Gender_3class_clf1')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained('../Gender_3class_clf1')\n",
    "text_classification_gender = pipeline('sentiment-analysis', model=model2, tokenizer=tokenizer2, device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "BS = 128\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_list) -> None:\n",
    "        super().__init__()\n",
    "        self.data = text_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_infer(text_list):\n",
    "    questions = [int(\"?\" in x or \"ï¼Ÿ\" in x) for x in text_list]\n",
    "    dataset = MyDataset(text_list)\n",
    "    genders = []\n",
    "    sentiments = []\n",
    "    for out in tqdm(text_classification_gender(dataset, batch_size=BS), total=len(dataset)):\n",
    "        genders.append(out['label'])\n",
    "    for out in tqdm(text_classification_sentiment(dataset, batch_size=BS), total=len(dataset)):\n",
    "        sentiments.append(out['label'])\n",
    "    return np.array(genders), np.array(sentiments), np.array(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['anger',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'happiness',\n",
    " 'like',\n",
    " 'none',\n",
    " 'sadness',\n",
    " 'surprise']\n",
    "\n",
    "def compute_metrics(pred_texts, gt_history, emotion_preds, sentiment_labels, emotion_labels, gender_preds, gender_labels, question_preds, question_labels):\n",
    "    ret = {}\n",
    "    ret[\"acc_emotions\"] = (emotion_preds == emotion_labels)\n",
    "    print(classification_report(emotion_labels, emotion_preds, target_names=emotion_list))\n",
    "    ret[\"acc_genders\"] = (gender_preds == gender_labels)\n",
    "    ret[\"acc_questions\"] = (question_preds == question_labels)\n",
    "    ret[\"avg_len\"] = [len(x) for x in pred_texts]\n",
    "    for k, v in ret.items():\n",
    "        ret[k] = np.mean(v)\n",
    "    ret[\"dist_2\"] = distinct_2(pred_texts)\n",
    "    ret[\"avg_acc\"] = (ret[\"acc_emotions\"] + ret[\"acc_genders\"] + ret[\"acc_questions\"]) / 3\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_texts = [line.strip() for line in open(\"../train_dulemon_outputs/bart_condition1/infer_robust1/topp_0.5.txt\")]\n",
    "gender_preds, emotion_preds, question_preds = batch_infer(pred_texts)\n",
    "compute_metrics(pred_texts, gt_history, emotion_preds, sentiment_labels, emotion_labels, gender_preds, gender_labels, question_preds, question_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a2c21b6ce3a9a25fbf96bbbc230c069683ea074939a9001cdcf1028939befc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
